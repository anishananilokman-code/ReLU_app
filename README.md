# ReLU_app
This Streamlit app visualizes the behavior of the ReLU (Rectified Linear Unit) activation function, which is one of the most commonly used activation functions in neural networks. ReLU is applied to the output of neurons in the hidden layers of deep neural networks to introduce non-linearity.
